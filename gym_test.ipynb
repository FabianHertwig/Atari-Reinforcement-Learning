{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The typical imports\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "import keras\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "# Reset it, returns the starting frame\n",
    "frames = []\n",
    "frame = env.reset()\n",
    "# Render\n",
    "#env.render()\n",
    "\n",
    "is_done = False\n",
    "while not is_done:\n",
    "  # Perform a random action, returns the new frame, reward and whether the game is over\n",
    "  frame, reward, is_done, _ = env.step(env.action_space.sample())\n",
    "  # Render\n",
    "  frames.append(env.render(mode = 'rgb_array'))\n",
    "env.close()\n",
    "display_frames_as_gif(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(downsample(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_reward(reward):\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(model, gamma, start_states, actions, rewards, next_states, is_terminal):\n",
    "    \"\"\"Do one deep Q learning iteration.\n",
    "    \n",
    "    Params:\n",
    "    - model: The DQN\n",
    "    - gamma: Discount factor (should be 0.99)\n",
    "    - start_states: numpy array of starting states\n",
    "    - actions: numpy array of one-hot encoded actions corresponding to the start states\n",
    "    - rewards: numpy array of rewards corresponding to the start states and actions\n",
    "    - next_states: numpy array of the resulting states corresponding to the start states and actions\n",
    "    - is_terminal: numpy boolean array of whether the resulting state is terminal\n",
    "    \n",
    "    \"\"\"\n",
    "    # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
    "    next_Q_values = model.predict([next_states, np.ones(actions.shape)])\n",
    "    # The Q values of the terminal states is 0 by definition, so override them\n",
    "    next_Q_values[is_terminal] = 0\n",
    "    # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
    "    # Fit the keras model. Note how we are passing the actions as the mask and multiplying\n",
    "    # the targets by the actions.\n",
    "    model.fit(\n",
    "        [start_states, actions], actions * Q_values[:, None],\n",
    "        epochs=1, batch_size=len(start_states), verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atari_model(n_actions):\n",
    "    # We assume a tensorflow backend here, so the \"channels\" are last.\n",
    "    ATARI_SHAPE = (105, 80, 4)\n",
    "\n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')\n",
    "    actions_input = keras.layers.Input((n_actions,), name='mask')\n",
    "\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)\n",
    "    \n",
    "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = keras.layers.convolutional.Conv2D(16, (8, 8), activation=\"relu\", strides=(4, 4))(normalized)\n",
    "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = keras.layers.convolutional.Conv2D(32, (4, 4), activation=\"relu\", strides=(2, 2))(conv_1)\n",
    "    # Flattening the second convolutional layer.\n",
    "    conv_flattened = keras.layers.core.Flatten()(conv_2)\n",
    "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "    hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = keras.layers.Dense(n_actions)(hidden)\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    filtered_output = keras.layers.merge([output, actions_input], mode='mul')\n",
    "\n",
    "    model = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
    "    optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "    model.compile(optimizer, loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Tool\n",
    "\n",
    "The idea behind experience replay is quite simple: at each Q-learning iteration, you play one step in the game, but instead of updating the model based on that last step, you add all the relevant information from the step you just took (current state, next state, action taken, reward and whether the next state is terminal) to a finite-size memory (of 1,000,000 elements in this case), and then call fit_batch on a sample of that memory (of 32 elements in our case). Before doing any iterations on the neural network, we prefill the memory with a random policy up to a certain number of elements (50,000 in our case).\n",
    "\n",
    "\n",
    "This ring buffer supports most of what you would expect (iteration, getting an arbitrary item etc.), but won’t work with random.sample. I recommend you simply implement a quick random sampling function for this. It shouldn’t be hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingBuf:\n",
    "    def __init__(self, size):\n",
    "        # Pro-tip: when implementing a ring buffer, always allocate one extra element,\n",
    "        # this way, self.start == self.end always means the buffer is EMPTY, whereas\n",
    "        # if you allocate exactly the right number of elements, it could also mean\n",
    "        # the buffer is full. This greatly simplifies the rest of the code.\n",
    "        self.data = [None] * (size + 1)\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        \n",
    "    def append(self, element):\n",
    "        self.data[self.end] = element\n",
    "        self.end = (self.end + 1) % len(self.data)\n",
    "        # end == start and yet we just added one element. This means the buffer has one\n",
    "        # too many element. Remove the first element by incrementing start.\n",
    "        if self.end == self.start:\n",
    "            self.start = (self.start + 1) % len(self.data)\n",
    "            \n",
    "    def add(self, state, action, new_frame, reward, is_done):\n",
    "        self.append((state, action, new_frame, reward, is_done))\n",
    "            \n",
    "    def sample_batch(self, length):\n",
    "        \"\"\"\n",
    "        Returns states_batch, action_batch, next_states_batch, reward_batch, done_batch\n",
    "        \"\"\"\n",
    "        if length > len(self):\n",
    "            samples =  self.data\n",
    "        else:\n",
    "            indices = np.random.randint(self.start, self.end, length)\n",
    "            samples =  [self.data[i] for i in indices]\n",
    "        return map(np.array, zip(*samples))\n",
    "            \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[(self.start + idx) % len(self.data)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.end < self.start:\n",
    "            return self.end + len(self.data) - self.start\n",
    "        else:\n",
    "            return self.end - self.start\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = RingBuf(10)\n",
    "for i in range(10):\n",
    "    test.append(np.array([i, i*2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, i2 = test.sample_batch(3)\n",
    "i, i2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some functions that you have to implement yourself when following the blog post. These are called in the `q_iteration` function. Also the `sample_batch` function got added to the `RingBuf` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_for_iteration(iteration):\n",
    "    if(iteration >= 1000000):\n",
    "        return 0.1\n",
    "    else:\n",
    "        return (-9e-7) * iteration + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_epsilon_for_iteration(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_epsilon_for_iteration(999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_epsilon_for_iteration(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_action(epsilon, model, state):\n",
    "    \n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    \n",
    "    state_ext = np.expand_dims(state, axis=0)\n",
    "    action_mask = np.ones((4,state.shape[-1]))\n",
    "    logits = model.predict([state_ext,action_mask])[0]\n",
    "    # TODO: do not use argmax but multinomal (needs a softmax layer?)\n",
    "    return np.argmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper suggests to input the last four frames to the network, so it can figure out what the speed, directetion and acceleration of an object is. At the beginning of each game, we will initialise a buffer with 4 times the very first frame. Each game step we will remove the oldest frame and append the new frame. To do that we will implement a framebuffer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame_Buffer:\n",
    "    def __init__(self, size, frame):\n",
    "        self.size = size\n",
    "        self.buffer = np.stack([frame] * self.size, axis = 2)\n",
    "        \n",
    "    def get(self):\n",
    "        return self.buffer\n",
    "    \n",
    "    def add(self, frame):\n",
    "        newest_3_frames = self.buffer[:,:,1:]\n",
    "        new_frame = np.expand_dims(frame, axis=2)\n",
    "        self.buffer = np.append(newest_3_frames, new_frame, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state = env.reset()\n",
    "state = preprocess(state)\n",
    "buffer = Frame_Buffer(4, state)\n",
    "new_state, *_ = env.step(1)\n",
    "new_state = preprocess(new_state)\n",
    "buffer.add(new_state)\n",
    "buffer.get().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is from the blogpost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_iteration(env, model, gamma, batch_size, iteration, state_buffer, replay_memory):\n",
    "    # Choose epsilon based on the iteration\n",
    "    epsilon = get_epsilon_for_iteration(iteration)\n",
    "    \n",
    "    action = choose_best_action(epsilon, model, state_buffer.get())\n",
    "    one_hot_action = keras.utils.np_utils.to_categorical(action, num_classes=env.action_space.n)\n",
    "\n",
    "    new_frame, reward, is_done, _ = env.step(action)\n",
    "    frames = state_buffer.get()\n",
    "    state_buffer.add(preprocess(new_frame))\n",
    "    memory.add(frames, one_hot_action, state_buffer.get(), reward, is_done)\n",
    "\n",
    "    # Sample and fit\n",
    "    states_batch, action_batch, next_states_batch, reward_batch, done_batch = memory.sample_batch(batch_size)\n",
    "    fit_batch(model, gamma, states_batch, action_batch, reward_batch, next_states_batch, done_batch)\n",
    "    \n",
    "    return is_done, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if GPU is available with tensorflow. If there is only a CPU device listed follow this instructions.\n",
    "\n",
    "If you already had tensorflow with GPU support setup once, it may help to reinstall tensorflow. For python 3.6 it is:\n",
    "\n",
    "    pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.5.0-cp36-cp36m-linux_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "MINIBATCH_SIZE = 32\n",
    "REPLAY_MEMORY_SIZE = 200000\n",
    "AGENT_HISTORY_LENGTH = 4\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "ACTION_REPEAT = 4\n",
    "REPLAY_START_SIZE = 50000\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "!mkdir videos\n",
    "monitor_path = \"./videos\"\n",
    "record_video_every = 20\n",
    "\n",
    "save_model_every = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "first_state = env.reset()\n",
    "frame_buffer = Frame_Buffer(4, preprocess(first_state))\n",
    "memory = RingBuf(1000000)\n",
    "model = atari_model(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Replay Memory with random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in tqdm(range(REPLAY_START_SIZE)):\n",
    "    action = env.action_space.sample()\n",
    "    one_hot_action = keras.utils.np_utils.to_categorical(action, num_classes=env.action_space.n)\n",
    "\n",
    "    new_frame, reward, is_done, _ = env.step(action)\n",
    "    frames = frame_buffer.get()\n",
    "    frame_buffer.add(preprocess(new_frame))\n",
    "    memory.add(frames, one_hot_action, frame_buffer.get(), reward, is_done)\n",
    "\n",
    "    if is_done:\n",
    "        first_state = env.reset()\n",
    "        frame_buffer = Frame_Buffer(4, preprocess(first_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model while playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = Monitor(env,\n",
    "                  directory=monitor_path,\n",
    "                  resume=True,\n",
    "                  video_callable=lambda count: count % record_video_every ==0)\n",
    "\n",
    "total_reward = 0\n",
    "reward_history = []\n",
    "iteration = 0\n",
    "for episode in  tqdm(range(EPISODES)):\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    first_state = env.reset()\n",
    "    frame_buffer = Frame_Buffer(4, preprocess(first_state))\n",
    "    while not is_done:\n",
    "        is_done, reward = q_iteration(env, model, DISCOUNT_FACTOR, MINIBATCH_SIZE, iteration, frame_buffer, memory)\n",
    "        total_reward += reward\n",
    "    print(\"Finished game: {} with reward: {}. Replay Memory Size: {}\".format(episode + 1, total_reward, len(memory)), end='\\r')\n",
    "    reward_history.append(total_reward)\n",
    "    if episode % save_model_every == 0:\n",
    "        model.save(\"./models/ep_{}.hdf5\".format(episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "model.save(\"./models/test.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "# Reset it, returns the starting frame\n",
    "frames = []\n",
    "frame = env.reset()\n",
    "\n",
    "frame_buffer = Frame_Buffer(4, preprocess(frame))\n",
    "\n",
    "is_done = False\n",
    "while not is_done:\n",
    "    action = choose_best_action(0.0, model, frame_buffer.get())\n",
    "\n",
    "    frame, reward, is_done, _ = env.step(env.action_space.sample())\n",
    "    # Render\n",
    "    frame_buffer.add(preprocess(frame))\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "env.close()\n",
    "display_frames_as_gif(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
