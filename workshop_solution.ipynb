{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung\n",
    "\n",
    "2013 zeigte DeepMind wie sie mit ein und denselben Machine Learning Algorithmus lernen können viele verschiedene Atari Spiele zu spielen. Diese Algorithmus ist das *Deep-Q-Learning*. Wie gut er Spiele lernen kann zeigt DeepMind gerne anhand von Breakout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"800\" height=\"560\" src=\"https://www.youtube.com/embed/TmPfTpjtdgg?rel=0&amp;showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Workshop werden wir diesen Algorithmus kennen lernen, zunächst für einige einfache Spiele umsetzen und uns zum Schluss Breakout vornehmen. \n",
    "\n",
    "Als erstes sehen wir uns das OpenAI Gym genauer an. Dies ist die Simulation in welcher neben den Atari Spielen auch physikalische Experimente durchgeführt werden können. Dann werden wir *Deep-Q-Learning* implementieren und lernen einen Stab zu balancieren oder ein doppeltes Pendel nach oben zu schwingen. Daraufhin werden wir noch zwei Verbesserungen implementieren, den *Experience Replay Memory* und *Double-DQN*. Damit lassen wir den Algorithmus lernen einen SpaceX Booster auf dem Droneship zu landen. Zuletzt setzen wir das *Deep* in *Deep-Q-Learning* um und erstellen ein Modell, das nur von den Pixeln des Spieles Atari Breakout lernt, dieses zu spielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Paper\n",
    "\n",
    "Den Deep-Q-Learning Algorithmus hat DeepMind in dem Paper [Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602) vorgestellt. Später wurde dieser mit der Double-DQN Verbesserung in einem Paper in dem Nature Magazin veröffentlicht: [Human-level control through deep reinforcement learning](http://dx.doi.org/10.1038/nature14236).\n",
    "\n",
    "#### Weiterführend\n",
    "Über die Jahre wurden viele kleine Weiterentwicklungen des DQN-Algorithmus vorgestellt. Die vielversprechenden sind in dem [Rainbow: Combining Improvements in Deep Reinforcement Learning](http://arxiv.org/abs/1710.02298) Paper zusammengefasst und dieses Paper ist sicher ein guter Einstiegspunkt, um sich in diese Weiterentwicklungen zu vertiefen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Jupyter Notebook Einführung\n",
    "- Python Einführung\n",
    "- Numpy Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir benutzen einige Packages. Bitte stell sicher, dass alle Packages wie in der Readme beschrieben installiert wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The gym\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "# The typical imports\n",
    "import numpy as np\n",
    "import itertools\n",
    "import glob\n",
    "import re\n",
    "import os, datetime\n",
    "\n",
    "\n",
    "# Deep Learning Tools\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import multiply\n",
    "\n",
    "# Displaying Tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "\n",
    "import tensorboard_logging\n",
    "# Utils from other notebooks\n",
    "import import_ipynb\n",
    "import Ringbuffer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das OpenAi Gym kennen lernen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OpenAI Gym](https://gym.openai.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = 'CartPole-v1'\n",
    "env = gym.make(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "# Reset it, returns the starting frame\n",
    "frames = []\n",
    "frame = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_done = False\n",
    "while not is_done:\n",
    "  # Perform a random action, returns the new frame, reward and whether the game is over\n",
    "  frame, reward, is_done, _ = env.step(env.action_space.sample())\n",
    "  # Render\n",
    "  frames.append(env.render(mode = 'rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selbst den Stab balancieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem folgenden Code kannst du selbst einmal probieren den Stab zu balancieren. Unter der Grafik werden zwei Buttons für die zwei möglichen Aktionen angezeigt. Jedes mal wenn du einen Button klickst, wird ein Schritt in der Simulation mit der entsprechenden Aktion durchgeführt. Sobald der Stab zu weit kippt wird die Simulation automatisch neu gestartet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def leftclicked(something):\n",
    "    \"\"\" Apply a force to the left of the cart\"\"\"\n",
    "    onclick(0)\n",
    "\n",
    "def rightclicked(something):\n",
    "    \"\"\" Apply a force to the right of the cart\"\"\"\n",
    "    onclick(1)\n",
    "    \n",
    "def display_buttons():\n",
    "    \"\"\" Display the buttons you can use to apply a force to the cart \"\"\"\n",
    "    left = widgets.Button(description=\"<\")\n",
    "    right = widgets.Button(description=\">\")\n",
    "    display(left, right)\n",
    "    \n",
    "    left.on_click(leftclicked)\n",
    "    right.on_click(rightclicked)\n",
    "\n",
    "# Create the environment and display the initial state\n",
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "firstframe = env.render(mode = 'rgb_array')\n",
    "fig,ax = plt.subplots()\n",
    "im = ax.imshow(firstframe) \n",
    "\n",
    "# Show the buttons to control the cart\n",
    "display_buttons()\n",
    "\n",
    "\n",
    "# Function that defines what happens when you click one of the buttons\n",
    "frames = []\n",
    "def onclick(action):\n",
    "    global frames\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frame = env.render(mode = 'rgb_array')\n",
    "    im.set_data(frame)\n",
    "    frames.append(frame)\n",
    "    print(\"Observation: {}. Reward: {}. Done: {}. Info: {}.\".format(observation, reward, done, info), end='\\r')\n",
    "    if done:\n",
    "        env.reset()\n",
    "        print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir uns deinen Balanceversuch noch etwas flüssiger anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "if frames:\n",
    "    utils.display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "Hier sollte ein grundlegendes Verständnis für den DQN Algorithmus und einige Begriffe verschafft werden.\n",
    "\n",
    "\n",
    "Diese vier evtl schon oben beim Gym.\n",
    "\n",
    "- agent\n",
    "- environment\n",
    "- action\n",
    "- state\n",
    "- episode\n",
    "\n",
    "\n",
    "- policy\n",
    "- discounting\n",
    "- optimal action value function Q*(s,a)\n",
    "- Bellman equation\n",
    "- model-free\n",
    "- e-greedy strategy\n",
    "- experience replay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Einzelteile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir in der Q-Learning Intution gesehen haben besteht der Algorithmus aus mehreren einzelteilen. Diese wollen wir nun implementieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Action Value  Function\n",
    "Oder auch das Machine Learning Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gym_model(learning_rate, n_actions, input_shape):\n",
    "    \n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = keras.layers.Input(input_shape, name='frames')\n",
    "    actions_input = keras.layers.Input((n_actions,), name='mask')\n",
    "\n",
    "    flat = keras.layers.Flatten()(frames_input)\n",
    "    hidden = keras.layers.Dense(16, activation='relu')(flat)\n",
    "    hidden = keras.layers.Dense(16, activation='relu')(hidden)\n",
    "    hidden = keras.layers.Dense(16, activation='relu')(hidden)\n",
    "    output = keras.layers.Dense(n_actions)(hidden)\n",
    "\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    filtered_output = multiply([output, actions_input])\n",
    "\n",
    "    model = keras.models.Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "    optimizer = optimizer=keras.optimizers.RMSprop(lr=learning_rate, rho=0.95, epsilon=0.01)\n",
    "    model.compile(optimizer, loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper [Human-level control through deep reinforcement\n",
    "learning](http://www.davidqiu.com:8888/research/nature14236.pdf) DeepMind suggests using a separate network to predict target values and only update that network every 10000 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model(model):\n",
    "    model_copy = keras.models.clone_model(model)\n",
    "    model_copy.set_weights(model.get_weights())\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounting mit der Bellman Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_bellman(rewards, next_Q_values, is_terminal, discount_factor):\n",
    "    return rewards + np.invert(is_terminal).astype(np.float32) * discount_factor * np.max(next_Q_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Epsilon Greedy Strategie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_for_iteration(iteration, decay_start_iteration, decay_end_iteration, max_q, min_q):\n",
    "    if(iteration <= decay_start_iteration):\n",
    "        return max_q\n",
    "    if(iteration >= decay_start_iteration + decay_end_iteration):\n",
    "        return min_q\n",
    "    else:\n",
    "        return ((min_q - max_q) / (decay_start_iteration + decay_end_iteration)) * iteration + max_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_epsilon_for_iteration(0, 0, 1000000, 1.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_epsilon_for_iteration(999999, 0, 1000000, 1.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_epsilon_for_iteration(1000000, 0,  1000000, 1.0, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Wahl der richtigen Aktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_action(model, state, action_space_size):\n",
    "    # Make a \"batch\" out of the state.\n",
    "    state_ext = np.expand_dims(state, axis=0)\n",
    "    # Create a mask consisting out of ones for the model imput.\n",
    "    action_mask = np.ones((state.shape[-1], action_space_size))\n",
    "    logits = model.predict([state_ext,action_mask])[0]\n",
    "    # TODO: do not use argmax but multinomal (needs a softmax layer?)\n",
    "    return np.argmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_choose_best_action(epsilon, model, state, action_space):\n",
    "    if np.random.random() < epsilon:\n",
    "        action = action_space.sample()\n",
    "    else:\n",
    "        action = choose_best_action(model, state, action_space.n)\n",
    "    return action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(action, action_space_size):\n",
    "    return keras.utils.np_utils.to_categorical(action, num_classes=action_space_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper suggests to input the last four frames to the network, so it can figure out what the speed, directetion and acceleration of an object is. At the beginning of each game, we will initialise a buffer with 4 times the very first frame. Each game step we will remove the oldest frame and append the new frame. To do that we will implement functions that help updating the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state_buffer(size, frame):\n",
    "    return np.stack([frame] * size, axis = 1)\n",
    "\n",
    "def update_state_buffer(state_buffer, new_frame):\n",
    "    return np.append(state_buffer[:,1:], np.expand_dims(new_frame, axis=1), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eine Q Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_iteration(env, model, discount_factor, batch_size, iteration, state, replay_memory, \n",
    "                epsilon_decay_start_iteration, epsilon_decay_end_iteration, start_epsilon=1.0, end_epsilon=0.1):\n",
    "\n",
    "    epsilon = get_epsilon_for_iteration(iteration, epsilon_decay_start_iteration, epsilon_decay_end_iteration, start_epsilon, end_epsilon)\n",
    "    \n",
    "    action = e_greedy_choose_best_action(epsilon, model, state, env.action_space)\n",
    "    one_hot_action = one_hot_encode(action, env.action_space.n)\n",
    "\n",
    "    new_frame, reward, is_done, _ = env.step(action)\n",
    "    new_frame = preprocess(new_frame)\n",
    "    next_state = update_state_buffer(state, new_frame)\n",
    "   \n",
    "    memory.add(state, one_hot_action, next_state, reward, is_done)\n",
    "    state = next_state\n",
    "    \n",
    "    states_batch, action_batch, next_states_batch, reward_batch, done_batch = memory.sample_batch(batch_size)\n",
    "    \n",
    "    next_Q_values_batch = model.predict([next_states_batch, np.ones(action_batch.shape)])\n",
    "    Q_values_batch = discount_bellman(reward_batch, next_Q_values_batch, done_batch, discount_factor)\n",
    "    \n",
    "    model.fit([states_batch, action_batch], action_batch * Q_values_batch[:, None],\n",
    "        epochs=1, batch_size=len(states_batch), verbose=0)\n",
    "    \n",
    "    return state, is_done, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "MINIBATCH_SIZE = 32\n",
    "REPLAY_MEMORY_SIZE = 50000 \n",
    "EPSILON_DECAY_LENGTH = 1000\n",
    "TARGET_NETWORK_UPDATE_INTERVAL = 100\n",
    "AGENT_HISTORY_LENGTH = 1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "REPLAY_START_SIZE = 10\n",
    "MAXIMUM_ITERATIONS = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment)\n",
    "memory = Ringbuffer.RingBuf(REPLAY_MEMORY_SIZE)\n",
    "model = gym_model(LEARNING_RATE, env.action_space.n, (env.observation_space.shape[0],AGENT_HISTORY_LENGTH))\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "logger = tensorboard_logging.Logger(logdir)\n",
    "iteration = 0\n",
    "start_episode = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model while playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reward_plot = utils.LivePlot(\"Total Reward per Game\")\n",
    "for episode in tqdm(itertools.count(start=start_episode), unit = \"episode\"):\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    first_state = preprocess(env.reset())\n",
    "    state = init_state_buffer(AGENT_HISTORY_LENGTH, first_state)\n",
    "\n",
    "    while not is_done:\n",
    "        state, is_done, reward = q_iteration(env, model, DISCOUNT_FACTOR, MINIBATCH_SIZE, iteration, state, memory, REPLAY_START_SIZE, EPSILON_DECAY_LENGTH)\n",
    "        iteration += 1\n",
    "        total_reward += reward\n",
    "\n",
    "    logger.log_scalar(\"total_reward\", total_reward, iteration)\n",
    "    reward_plot.add(total_reward)\n",
    "\n",
    "    if iteration >= MAXIMUM_ITERATIONS:\n",
    "            break\n",
    "            \n",
    "reward_plot.finalise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "reward_plot.finalise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment)\n",
    "# Reset it, returns the starting frame\n",
    "frames = []\n",
    "frame = env.reset()\n",
    "\n",
    "frame_buffer = init_state_buffer(AGENT_HISTORY_LENGTH, preprocess(frame))\n",
    "\n",
    "is_done = False\n",
    "while not is_done:\n",
    "    action = choose_best_action(model, frame_buffer, env.action_space.n)\n",
    "\n",
    "    frame, reward, is_done, _ = env.step(action)\n",
    "    # Render\n",
    "    frame_buffer = update_state_buffer(frame_buffer, preprocess(frame))\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "env.close()\n",
    "utils.display_frames_as_gif(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_iteration_with_target_model(env, model, target_model, discount_factor, batch_size, iteration, state, replay_memory, \n",
    "                epsilon_decay_start_iteration, epsilon_decay_end_iteration, start_epsilon=1.0, end_epsilon=0.1):\n",
    "\n",
    "    epsilon = get_epsilon_for_iteration(iteration, epsilon_decay_start_iteration, epsilon_decay_end_iteration, start_epsilon, end_epsilon)\n",
    "    \n",
    "    action = e_greedy_choose_best_action(epsilon, model, state, env.action_space)\n",
    "    one_hot_action = one_hot_encode(action, env.action_space.n)\n",
    "\n",
    "    new_frame, reward, is_done, _ = env.step(action)\n",
    "    new_frame = preprocess(new_frame)\n",
    "    next_state = update_state_buffer(state, new_frame)\n",
    "   \n",
    "    memory.add(state, one_hot_action, next_state, reward, is_done)\n",
    "    state = next_state\n",
    "    \n",
    "    states_batch, action_batch, next_states_batch, reward_batch, done_batch = memory.sample_batch(batch_size)\n",
    "    \n",
    "    next_Q_values_batch = target_model.predict([next_states_batch, np.ones(action_batch.shape)])\n",
    "    Q_values_batch = discount_bellman(reward_batch, next_Q_values_batch, done_batch, discount_factor)\n",
    "    \n",
    "    model.fit([states_batch, action_batch], action_batch * Q_values_batch[:, None],\n",
    "        epochs=1, batch_size=len(states_batch), verbose=0)\n",
    "    \n",
    "    return state, is_done, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "MINIBATCH_SIZE = 32\n",
    "REPLAY_MEMORY_SIZE = 50000 \n",
    "EPSILON_DECAY_LENGTH = 1000\n",
    "TARGET_NETWORK_UPDATE_INTERVAL = 300\n",
    "AGENT_HISTORY_LENGTH = 1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "REPLAY_START_SIZE = 10\n",
    "MAXIMUM_ITERATIONS = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment)\n",
    "memory = Ringbuffer.RingBuf(REPLAY_MEMORY_SIZE)\n",
    "model = gym_model(LEARNING_RATE, env.action_space.n, (env.observation_space.shape[0],AGENT_HISTORY_LENGTH))\n",
    "target_model = copy_model(model)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "logger = tensorboard_logging.Logger(logdir)\n",
    "iteration = 0\n",
    "start_episode = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model while playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reward_plot = utils.LivePlot(\"Total Reward per Game\")\n",
    "for episode in tqdm(itertools.count(start=start_episode), unit = \"episode\"):\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    first_state = preprocess(env.reset())\n",
    "    state = init_state_buffer(AGENT_HISTORY_LENGTH, first_state)\n",
    "\n",
    "    while not is_done:\n",
    "        state, is_done, reward = \\\n",
    "            q_iteration_with_target_model(env, model, target_model, DISCOUNT_FACTOR, MINIBATCH_SIZE, iteration, state, memory,REPLAY_START_SIZE, EPSILON_DECAY_LENGTH)\n",
    "        iteration += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        if iteration % TARGET_NETWORK_UPDATE_INTERVAL == 0:\n",
    "            target_model = copy_model(model)\n",
    "\n",
    "    logger.log_scalar(\"total_reward\", total_reward, iteration)\n",
    "    reward_plot.add(total_reward)\n",
    "\n",
    "    if iteration >= MAXIMUM_ITERATIONS:\n",
    "            break\n",
    "            \n",
    "reward_plot.finalise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_plot.finalise()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment)\n",
    "# Reset it, returns the starting frame\n",
    "frames = []\n",
    "frame = env.reset()\n",
    "\n",
    "frame_buffer = init_state_buffer(4, preprocess(frame))\n",
    "\n",
    "is_done = False\n",
    "while not is_done:\n",
    "    action = choose_best_action(model, frame_buffer, env.action_space.n)\n",
    "\n",
    "    frame, reward, is_done, _ = env.step(action)\n",
    "    # Render\n",
    "    frame_buffer = update_state_buffer(frame_buffer, preprocess(frame))\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "env.close()\n",
    "utils.display_frames_as_gif(frames)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
